Q1.
Value iteration  10 max_iterations
Policy iteration 5 max_iterations

Q2.
 (check "Plots" folder)

Q3.
Small Lake:
    Sarsa Control:
        max_episodes = 1400
        eta = 0.3
        epsilon = 0.7

    Q Learning Control:
        max_episodes = 1500
        eta = 0.3
        epsilon = 0.7
  
At higher values of Learning rate we can see that it requires more episodes to compute the optimal policy till 0.6 , then the optimal policy for any number of episodes wont be produced. At epsilon = 0.7 , and with just 1400 episodes we are able to get a consistent policy that is optimal. At lower epsilon values, we have to go for higher episodes as then exploration would be reduced, but at very high epsilons we see that only exploitation occurs.  
Here sarsa at some seeds can provide optimal paths( but not policies), the testing was done by using a set of learning rates in a for loop and checking which had better policies. Then further testing was done to see the select vlaues shwoing any changes or breaks by repeating the function a few times. These stages after compiling we are able to find that 0.3 and 0.7 was the best combination for learning rate and epsilon respectively.As mentioned before, if given a random seed we are able to generate the same optimal results for even lower episodes.
The big lake was not compiled successfully as every iteration of the various learning rates and epsilons did not prove optimal to the big lake. Even as far increasing the number of episodes well above 6000 did not prove useful, this can be due to the fact the complexity of reaching the goal state in a very large gridworld makesit difficult to compute and optimal path by the Model-Free methods, this causes the algorithm to fail.


Q4.
In linear action-value function approximation, the parameter vector θ is a weight vector that is used to combine the feature vectors for each possible pair of state s and action a. Each element of θ corresponds to a particular feature in the feature vectors, and the value of that element determines the weight that is given to that feature when the feature vector is combined with the rest of the feature vectors to approximate the action-value function.

Tabular model-free reinforcement learning algorithms are a special case of non-tabular model-free reinforcement learning algorithms because they rely on a table to store and update the estimates of the action-value function. In a tabular model-free reinforcement learning algorithm, the estimates of the action-value function are stored in a table, and the table is updated at each time step based on the observed state and action and the resulting reward.
On the other hand, non-tabular model-free reinforcement learning algorithms do not rely on a table to store and update the estimates of the action-value function. Instead, they use function approximation techniques, such as linear function approximation or neural network function approximation, to approximate the action-value function. In these algorithms, the estimates of the action-value function are not stored in a table, but are represented as a set of parameters in a parametric function approximator. The parameters of the function approximator are updated at each time step based on the observed state and action and the resulting reward.

Q5.

If the agent were to always act greedily with respect to Q, it would only select the actions that it thinks will lead to the highest reward based on its current knowledge. This might lead the agent to become stuck in a suboptimal policy, whereas  if the agent were to always act randomly, it would not make use of its current knowledge and would not be able to learn about the consequences of its actions. This would make it difficult for the agent to learn an optimal policy.
By using an ε-greedy policy, the agent is able to balance the trade-off between exploration and exploitation, allowing it to learn about the consequences of its actions while still making use of its current knowledge. This can help the agent learn an optimal policy more efficiently.

Q6.
The authors of the DQN paper argue that using a target Q-network can help reduce the correlations between the action-values and the target values, which can lead to more stable learning. They also note that using a target Q-network can help reduce the risk of the learning process becoming destabilized due to the changing nature of the online Q-network's estimates. Overall, the use of a target Q-network can help improve the performance of the DQN algorithm, leading to better solutions to the reinforcement learning problem.